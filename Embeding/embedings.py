# -*- coding: utf-8 -*-
"""Embedings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sGsLwChfb1QnV5Cg55CX3-N2G3Tcp3Y5

### Install Required Librires
"""

!pip install langchain langchain-google-genai chromadb google-generativeai

!pip install --upgrade chromadb

!pip install -U langchain-community

!pip install --upgrade langchain langchain-google-genai

"""### Import required librires"""

import os
import shutil
import sqlite3
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings,ChatGoogleGenerativeAI
# from langchain_google_genai import GoogleGenerativeAI
# from langchain.utilities import RunnableLambda
from langchain_community.vectorstores import Chroma
from chromadb.config import Settings
from langchain.chains import RetrievalQA
import chromadb

from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain.chains import LLMChain

from langchain.schema.output_parser import StrOutputParser
from langchain.prompts.chat import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

"""#### Extacting Google Api Key"""

from google.colab import userdata
api_key=userdata.get('genai1')

os.environ['GOOGLE_API_KEY']=api_key

"""#### Extracting data From Data Base"""

conn = sqlite3.connect('/content/itc_documents1.db')
cursor = conn.cursor()

# Read rows
cursor.execute("SELECT id, content, source FROM documents")
rows = cursor.fetchall()

"""#### Convert to LangChain Document objects"""

documents = [
    Document(page_content=row[1], metadata={"id": row[0], "source": row[2]})
    for row in rows
]

"""#### Chunk the Documents"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunked_docs = text_splitter.split_documents(documents)

"""####  Initialize Embeddings & Vector Store"""

embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

vectorstore = Chroma.from_documents(
    documents=chunked_docs,
    embedding=embedding,
    persist_directory="/content/chroma_db1"
)
vectorstore.persist()

"""#### Saving the file"""

shutil.make_archive("/content/chroma_db1", 'zip', "/content/chroma_db1")

"""#### Load LLM (Gemini)"""

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.2
)

"""#### Build Retrieval-QA Chain"""

retriever = vectorstore.as_retriever(search_type = "mmr",search_kwargs={"k": 5})

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

"""#### Ask Questions (Query the Chain)"""

query = "Is ITCâ€™s revenue trending upward (2023 vs. 2024)?"
response = qa_chain({"query": query})

print("Answer:")
print(response["result"])
print("\nSources:")
for doc in response["source_documents"]:
    print(doc.metadata)