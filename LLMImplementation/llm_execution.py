# -*- coding: utf-8 -*-
"""LLM Execution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xxjIsyOSECDIkL9hjJMADc3Csd5aFK9t

### Install Required Librires
"""

!pip install langchain langchain-google-genai chromadb google-generativeai
!pip install --upgrade chromadb
!pip install -U langchain-community
!pip install --upgrade langchain langchain-google-genai

"""### Import required librires"""

import os
from langchain_community.vectorstores import Chroma

from langchain_google_genai import GoogleGenerativeAI

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings,ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from chromadb.config import Settings
from langchain.chains import RetrievalQA
import chromadb

from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain.chains import LLMChain

from langchain.schema.output_parser import StrOutputParser
from langchain.prompts.chat import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

""" Load Google API Key"""

from google.colab import userdata
api_key=userdata.get('genai1')

os.environ['GOOGLE_API_KEY']=api_key

"""Unzip the folder after uploading"""

# Unzip the folder after uploading
import zipfile

with zipfile.ZipFile('/content/chroma_db1.zip', 'r') as zip_ref:
    zip_ref.extractall('chroma_db2')

""" Load Chroma Vectorstore"""

embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

vectorstore = Chroma(
    persist_directory='chroma_db2',
    embedding_function=embedding
)

vectorstore.persist()

"""Creating a retriever"""

mmr_retriever = vectorstore.as_retriever(
    search_type = "mmr",
    search_kwargs = {"k":3, "lambda_mult":1}
)

"""Helper Functions"""

# Helper Functions
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def get_docs_and_context(question):
    docs = mmr_retriever.get_relevant_documents(question)
    return {"question": question, "docs": docs, "context": format_docs(docs)}

parallel_chain = RunnableLambda(lambda x: {
    "question": x["question"],
    **get_docs_and_context(x["question"])
})

""" Create Prompt Template"""

chat_prompt = ChatPromptTemplate.from_messages([
    ("system",
     """You are a domain-specific AI financial analyst focused on company-level performance evaluation.

Your task is to analyze and respond to user financial queries strictly based on the provided transcript data: {context}.

Rules:
1. ONLY extract facts, figures, and insights that are explicitly available in the transcript.
2. If data is missing or partially available, clearly state: "The required data is not available in the current transcript." Then provide a generic but relevant explanation based on standard financial principles.
3. Maintain numerical accuracy and avoid interpretation beyond data boundaries.
4. Prioritize answers relevant to ITC Ltd., but keep response format adaptable to other firms and fiscal years.
5. Clearly present year-wise or metric-wise insights using bullet points or structured formats if applicable.

Your goals:
- Ensure 100% fidelity to source transcript.
- Do not assume or hallucinate missing numbers.
- Use clear, reproducible reasoning steps (e.g., show which line items support your conclusion).
- Output should be modular enough to scale across other companies and time periods.

Respond only to this question from the user."""),

    ("human", "{question}")
])

"""Gemini LLM and parser"""

# Step 3: Gemini LLM and parser
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=1
)

"""
Final Chain"""

parser = StrOutputParser()

# Final Chain
main_chain = (
    parallel_chain |
    RunnableLambda(lambda x: {
        "llm_input": {"question": x["question"], "context": x["context"]},
        "docs": x["docs"]
    }) |
    RunnableLambda(lambda x: {
        "result": (chat_prompt | llm | parser).invoke(x["llm_input"]),
        "source_documents": x["docs"]
    })
)

"""Run Query"""

# Query
query = "Is ITCâ€™s revenue trending upward (2023 vs. 2024)?"
output = main_chain.invoke({"question": query})

# Output
print("\nAnswer:")
print(output["result"])
print("\nSources:")
for doc in output["source_documents"]:
    print(doc.metadata)

